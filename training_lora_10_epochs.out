nohup: ignoring input
2025-04-10 13:18:09.532762: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-10 13:18:09.640873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-10 13:18:12.020587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/project/fyp24_ho3/tmtong/miniconda3/envs/muxit/lib/python3.11/site-packages/transformers/models/encodec/modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.register_buffer("padding_total", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)
Applying LoRA with r=16, alpha=32.0, dropout=0.1
Target modules for LoRA: ['out_proj']
Added LoRA modules! Trainable params: 2,359,296 (2.083333% of total 113,246,208)
Tuning language model with LoRA
Epoch: 0/10, Batch: 0/5666, Loss: 4.892578, Grad Norm: 0.352113
Epoch: 0/10, Batch: 1/5666, Loss: 4.490609, Grad Norm: 0.279031
Epoch: 0/10, Batch: 2/5666, Loss: 4.108094, Grad Norm: 0.374930
Epoch: 0/10, Batch: 3/5666, Loss: 4.709674, Grad Norm: 0.311632
Epoch: 0/10, Batch: 4/5666, Loss: 3.628158, Grad Norm: 0.311206
Epoch: 0/10, Batch: 5/5666, Loss: 3.525675, Grad Norm: 0.330829
Epoch: 0/10, Batch: 6/5666, Loss: 3.142745, Grad Norm: 0.488043
Epoch: 0/10, Batch: 7/5666, Loss: 4.540822, Grad Norm: 0.310835
Epoch: 0/10, Batch: 8/5666, Loss: 2.815220, Grad Norm: 0.449947
Epoch: 0/10, Batch: 9/5666, Loss: 4.805210, Grad Norm: 0.320323
Epoch: 0/10, Batch: 10/5666, Loss: 4.324750, Grad Norm: 0.361963
Epoch: 0/10, Batch: 11/5666, Loss: 5.258954, Grad Norm: 0.378582
Epoch: 0/10, Batch: 12/5666, Loss: 2.729014, Grad Norm: 0.310855
Epoch: 0/10, Batch: 13/5666, Loss: 3.301200, Grad Norm: 0.408653
Epoch: 0/10, Batch: 14/5666, Loss: 3.216322, Grad Norm: 0.283579
Epoch: 0/10, Batch: 15/5666, Loss: 4.823819, Grad Norm: 0.345050
Epoch: 0/10, Batch: 16/5666, Loss: 4.875947, Grad Norm: 0.312192
Epoch: 0/10, Batch: 17/5666, Loss: 3.162962, Grad Norm: 0.293161
Epoch: 0/10, Batch: 18/5666, Loss: 5.120352, Grad Norm: 0.334236
Epoch: 0/10, Batch: 19/5666, Loss: 2.661894, Grad Norm: 0.387354
Epoch: 0/10, Batch: 20/5666, Loss: 1.339377, Grad Norm: 0.391651
Epoch: 0/10, Batch: 21/5666, Loss: 3.607922, Grad Norm: 0.322052
Epoch: 0/10, Batch: 22/5666, Loss: 3.248819, Grad Norm: 0.306692
Epoch: 0/10, Batch: 23/5666, Loss: 4.156399, Grad Norm: 0.368030
Epoch: 0/10, Batch: 24/5666, Loss: 2.936976, Grad Norm: 0.341769
Epoch: 0/10, Batch: 25/5666, Loss: 3.823460, Grad Norm: 0.419820
Epoch: 0/10, Batch: 26/5666, Loss: 3.677278, Grad Norm: 0.269173
Epoch: 0/10, Batch: 27/5666, Loss: 4.704458, Grad Norm: 0.306937
Epoch: 0/10, Batch: 28/5666, Loss: 3.622520, Grad Norm: 0.326461
Epoch: 0/10, Batch: 29/5666, Loss: 4.020034, Grad Norm: 0.337380
Epoch: 0/10, Batch: 30/5666, Loss: 3.928659, Grad Norm: 0.339739
Epoch: 0/10, Batch: 31/5666, Loss: 4.730234, Grad Norm: 0.311442
Epoch: 0/10, Batch: 32/5666, Loss: 5.146825, Grad Norm: 0.350415
Epoch: 0/10, Batch: 33/5666, Loss: 4.992109, Grad Norm: 0.319483
Audio too short: 13.98s < 30s
Epoch: 0/10, Batch: 35/5666, Loss: 2.875755, Grad Norm: 0.349772
Epoch: 0/10, Batch: 36/5666, Loss: 4.200249, Grad Norm: 0.283440
Epoch: 0/10, Batch: 37/5666, Loss: 3.906825, Grad Norm: 0.333625
Epoch: 0/10, Batch: 38/5666, Loss: 3.177693, Grad Norm: 0.398812
Epoch: 0/10, Batch: 39/5666, Loss: 1.348277, Grad Norm: 0.636411
Epoch: 0/10, Batch: 40/5666, Loss: 3.806020, Grad Norm: 0.345214
Epoch: 0/10, Batch: 41/5666, Loss: 4.220186, Grad Norm: 0.346473
Epoch: 0/10, Batch: 42/5666, Loss: 3.335253, Grad Norm: 0.395974
Epoch: 0/10, Batch: 43/5666, Loss: 4.067463, Grad Norm: 0.282504
Epoch: 0/10, Batch: 44/5666, Loss: 2.444127, Grad Norm: 0.378221
Epoch: 0/10, Batch: 45/5666, Loss: 4.044552, Grad Norm: 0.522605
Epoch: 0/10, Batch: 46/5666, Loss: 2.797176, Grad Norm: 0.299177
Epoch: 0/10, Batch: 47/5666, Loss: 4.659225, Grad Norm: 0.337380
Converted mono audio to stereo. New shape: torch.Size([2, 6843664])
Epoch: 0/10, Batch: 48/5666, Loss: 2.818208, Grad Norm: 0.442313
Epoch: 0/10, Batch: 49/5666, Loss: 4.481141, Grad Norm: 0.322619
Epoch: 0/10, Batch: 50/5666, Loss: 4.701951, Grad Norm: 0.359439
Epoch: 0/10, Batch: 51/5666, Loss: 3.696947, Grad Norm: 0.353146
Epoch: 0/10, Batch: 52/5666, Loss: 4.696240, Grad Norm: 0.362616
Epoch: 0/10, Batch: 53/5666, Loss: 4.313525, Grad Norm: 0.307111
Epoch: 0/10, Batch: 54/5666, Loss: 1.053760, Grad Norm: 1.252062
Epoch: 0/10, Batch: 55/5666, Loss: 3.256514, Grad Norm: 0.314460
Epoch: 0/10, Batch: 56/5666, Loss: 2.425181, Grad Norm: 0.371229
Epoch: 0/10, Batch: 57/5666, Loss: 3.078191, Grad Norm: 0.375803
Epoch: 0/10, Batch: 58/5666, Loss: 3.272971, Grad Norm: 0.401133
Epoch: 0/10, Batch: 59/5666, Loss: 4.041219, Grad Norm: 0.325336
Epoch: 0/10, Batch: 60/5666, Loss: 4.875140, Grad Norm: 0.302719
Epoch: 0/10, Batch: 61/5666, Loss: 4.715446, Grad Norm: 0.319819
Epoch: 0/10, Batch: 62/5666, Loss: 4.774663, Grad Norm: 0.298678
Epoch: 0/10, Batch: 63/5666, Loss: 4.363508, Grad Norm: 0.322528
Epoch: 0/10, Batch: 64/5666, Loss: 3.667225, Grad Norm: 0.309493
Epoch: 0/10, Batch: 65/5666, Loss: 4.764791, Grad Norm: 0.324392
Epoch: 0/10, Batch: 66/5666, Loss: 3.645487, Grad Norm: 0.368622
Epoch: 0/10, Batch: 67/5666, Loss: 3.425491, Grad Norm: 0.345581
Epoch: 0/10, Batch: 68/5666, Loss: 4.186661, Grad Norm: 0.450965
Epoch: 0/10, Batch: 69/5666, Loss: 4.046886, Grad Norm: 0.291666
Epoch: 0/10, Batch: 70/5666, Loss: 4.340961, Grad Norm: 0.376004
Epoch: 0/10, Batch: 71/5666, Loss: 4.753670, Grad Norm: 0.398607
Epoch: 0/10, Batch: 72/5666, Loss: 5.302683, Grad Norm: 0.327310
Epoch: 0/10, Batch: 73/5666, Loss: 2.420738, Grad Norm: 0.370373
Epoch: 0/10, Batch: 74/5666, Loss: 4.674152, Grad Norm: 0.403947
Epoch: 0/10, Batch: 75/5666, Loss: 4.798968, Grad Norm: 0.338216
Epoch: 0/10, Batch: 76/5666, Loss: 3.196843, Grad Norm: 0.378392
Epoch: 0/10, Batch: 77/5666, Loss: 5.246288, Grad Norm: 0.368560
Epoch: 0/10, Batch: 78/5666, Loss: 4.448999, Grad Norm: 0.370150
Epoch: 0/10, Batch: 79/5666, Loss: 4.599168, Grad Norm: 0.339053
Epoch: 0/10, Batch: 80/5666, Loss: 5.617479, Grad Norm: 0.303050
Epoch: 0/10, Batch: 81/5666, Loss: 3.259701, Grad Norm: 0.444628
Epoch: 0/10, Batch: 82/5666, Loss: 3.800856, Grad Norm: 0.320967
Epoch: 0/10, Batch: 83/5666, Loss: 3.992066, Grad Norm: 0.420744
Epoch: 0/10, Batch: 84/5666, Loss: 4.391591, Grad Norm: 0.342012
Epoch: 0/10, Batch: 85/5666, Loss: 1.855395, Grad Norm: 0.525609
Epoch: 0/10, Batch: 86/5666, Loss: 1.243234, Grad Norm: 0.260141
Converted mono audio to stereo. New shape: torch.Size([2, 5014704])
Epoch: 0/10, Batch: 87/5666, Loss: 3.774738, Grad Norm: 0.470866
Converted mono audio to stereo. New shape: torch.Size([2, 3363736])
Epoch: 0/10, Batch: 88/5666, Loss: 4.268665, Grad Norm: 0.327195
Epoch: 0/10, Batch: 89/5666, Loss: 4.379832, Grad Norm: 0.289430
Epoch: 0/10, Batch: 90/5666, Loss: 4.715547, Grad Norm: 0.374293
Epoch: 0/10, Batch: 91/5666, Loss: 4.426944, Grad Norm: 0.350348
Epoch: 0/10, Batch: 92/5666, Loss: 3.551964, Grad Norm: 0.394119
Epoch: 0/10, Batch: 93/5666, Loss: 3.857346, Grad Norm: 0.316781
Epoch: 0/10, Batch: 94/5666, Loss: 3.071598, Grad Norm: 0.395722
Epoch: 0/10, Batch: 95/5666, Loss: 4.212918, Grad Norm: 0.311564
Converted mono audio to stereo. New shape: torch.Size([2, 5291364])
Epoch: 0/10, Batch: 96/5666, Loss: 4.119863, Grad Norm: 0.482546
Epoch: 0/10, Batch: 97/5666, Loss: 2.873372, Grad Norm: 0.340521
Epoch: 0/10, Batch: 98/5666, Loss: 3.155391, Grad Norm: 0.339208
Epoch: 0/10, Batch: 99/5666, Loss: 4.231853, Grad Norm: 0.317027
Epoch: 0/10, Batch: 100/5666, Loss: 4.778836, Grad Norm: 0.313372
Epoch: 0/10, Batch: 101/5666, Loss: 4.414974, Grad Norm: 0.359798
Epoch: 0/10, Batch: 102/5666, Loss: 3.668596, Grad Norm: 0.343213
Epoch: 0/10, Batch: 103/5666, Loss: 2.656188, Grad Norm: 0.316265
Epoch: 0/10, Batch: 104/5666, Loss: 2.960743, Grad Norm: 0.259613
Epoch: 0/10, Batch: 105/5666, Loss: 4.600371, Grad Norm: 0.378422
Epoch: 0/10, Batch: 106/5666, Loss: 3.718445, Grad Norm: 0.317926
Epoch: 0/10, Batch: 107/5666, Loss: 4.729743, Grad Norm: 0.291832
Epoch: 0/10, Batch: 108/5666, Loss: 3.528822, Grad Norm: 0.329598
Epoch: 0/10, Batch: 109/5666, Loss: 4.098160, Grad Norm: 0.317085
Epoch: 0/10, Batch: 110/5666, Loss: 3.235110, Grad Norm: 0.396823
Epoch: 0/10, Batch: 111/5666, Loss: 2.355976, Grad Norm: 0.325943
Epoch: 0/10, Batch: 112/5666, Loss: 3.728528, Grad Norm: 0.450993
Epoch: 0/10, Batch: 113/5666, Loss: 4.363687, Grad Norm: 0.331679
Epoch: 0/10, Batch: 114/5666, Loss: 2.925732, Grad Norm: 0.265317
Epoch: 0/10, Batch: 115/5666, Loss: 4.177617, Grad Norm: 0.308732
Audio too short: 1.28s < 30s
Epoch: 0/10, Batch: 117/5666, Loss: 4.410706, Grad Norm: 0.309687
Audio too short: 10.89s < 30s
Converted mono audio to stereo. New shape: torch.Size([2, 8142681])
Epoch: 0/10, Batch: 119/5666, Loss: 3.540659, Grad Norm: 0.255051